import whisper
import os
import torch
import gc  # f√ºr Speicherfreigabe

# Whisper-Cache-Verzeichnis setzen (falls nicht vorhanden)
cache_dir = "C:/Users/flori/openai_whisper_chache/"
os.makedirs(cache_dir, exist_ok=True)
os.environ["XDG_CACHE_HOME"] = cache_dir

# Modellwahl
model_name = "base"  # Alternativen: "tiny", "small", "medium", "large"
device = "cuda" if torch.cuda.is_available() else "cpu"
model = whisper.load_model(model_name, device=device)

def transcribe_audio(audio_path):
    try:
        print(f"üìÅ Starte Transkription auf Ger√§t: {device.upper()} ...")
        
        result = model.transcribe(audio_path, fp16=(device == "cuda"))
        
        text = result.get("text", "").strip()
        print("‚úÖ Transkription erfolgreich.")
        return text

    except Exception as e:
        return f"‚ùå Fehler bei Transkription: {str(e)}"
    
    finally:
        # üîÅ Speicher aufr√§umen (optional, vor allem bei vielen Durchl√§ufen)
        gc.collect()

